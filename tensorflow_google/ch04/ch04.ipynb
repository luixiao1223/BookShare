{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 深度学习与深层神经网路\n",
    "\n",
    "1. 深度学习就是深层神经网络的代名词\n",
    "2. 深度学习最重要的两个特性\n",
    "   + 多层\n",
    "   + 非线性\n",
    "\n",
    "## 线性模型的局限性\n",
    "\n",
    "最早的神经网络采用线性模型\n",
    "\n",
    "$$\n",
    "y=\\sum_{i}w_ix_i+b\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(1)}=xW^{(1)},y=a^{(1)}W^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y=(xW^{(1)})W^{(2)}\\rightarrow y=x(W^{(1)}W^{(2)})=xW^{'}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y=xW^{'}=\\left[\n",
    "      \\begin{array}{cc}\n",
    "      x_1 & x_2\n",
    "      \\end{array}\n",
    "      \\right]\\left[\n",
    "          \\begin{array}{c}\n",
    "          W_1^{'} \\\\\n",
    "  W_2^{'}\n",
    "          \\end{array}\n",
    "          \\right]=\\left[\n",
    "              \\begin{array}{cc}\n",
    "              W_{1}^{'}x_1 & W_2^{'} x_2\n",
    "              \\end{array}\n",
    "              \\right]\n",
    "$$\n",
    "\n",
    "线性模型不能解决异或问题。\n",
    "\n",
    "![异或](./xor.jpg)\n",
    "\n",
    "## 激活函数实现去线性化\n",
    "\n",
    "如何做的？\n",
    "\n",
    "![加入非线性的激活函数图](./nonlinear.png)\n",
    "\n",
    "$$\n",
    "\\begin{aligned} A_{1} &=\\left[a_{11}, a_{12}, a_{13}\\right]=f\\left(x W^{(1)}+b\\right)=f\\left(\\left[x_{1}, x_{2}\\right]\\left[\\begin{array}{ccc}{W_{1,1}^{(1)}} & {W_{1,2}^{(1)}} & {W_{1,3}^{(1)}} \\\\ {W_{2,1}^{(1)}} & {W_{2,2}^{(1)}} & {W_{2,3}^{(1)}}\\end{array}\\right]+\\left[\\begin{array}{lll}{b_{1}} & {b_{2}} & {b_{3}}\\end{array}\\right]\\right) \\\\ &=f\\left(\\left[W_{1,1}^{(1)} x_{1}+W_{2,1}^{(1)} x_{2}+b_{1}, W_{1,2}^{(1)} x_{1}+W_{2,2}^{(1)} x_{2}+b_{2}, W_{1,3}^{(1)} x_{1}+W_{2,3}^{(1)} x_{2}+b_{3}\\right]\\right) \\\\ &=\\left[f\\left(W_{1,1}^{(1)} x_{1}+W_{2,1}^{(1)} x_{2}+b_{1}\\right), f\\left(W_{1,2}^{(1)} x_{1}+W_{2,2}^{(1)} x_{2}+b_{2}\\right), f\\left(W_{1,3}^{(1)} x_{1}+W_{2,3}^{(1)} x_{2}+b_{3}\\right)\\right] \\end{aligned}\n",
    "$$\n",
    "\n",
    "常用的激活函数\n",
    "\n",
    "![常用的激活函数](./act.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.nn.relu(tf.matmul(x,w1)+base1)\n",
    "b = tf.nn.relu(tf.matmul(a,w2)+base2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "## 多层神经网络解决异或语言\n",
    "\n",
    "感知机理论上不可以的原因？（这个可以列一个专题来讲）FIXME 参考书籍《Perceptions:An Introtudction to Computational Geometry》 MIT Press,1969\n",
    "\n",
    "![perception](./perc.png)\n",
    "\n",
    "![deep](./deep.png)\n",
    "\n",
    "可以看到通过隐藏层，我们可以抽象出更为高维的信息。这些信息就可以用来分类数据。从而得到更好的分类结果。\n",
    "\n",
    "# 损失函数\n",
    "\n",
    "## 经典损失函数\n",
    "\n",
    "\n",
    "### 分类问题\n",
    "\n",
    "神经网络如何输出多分类问题。比如3分类问题。苹果、香蕉、梨。\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{苹果}=\\left(\n",
    "    \\begin{array}{c}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "    \\end{array}\n",
    "    \\right),\\mbox{香蕉}=\\left(\n",
    "        \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        1 \\\\\n",
    "        0\n",
    "        \\end{array}\n",
    "        \\right),\\mbox{梨}=\\left(\n",
    "            \\begin{array}{c}\n",
    "            0 \\\\\n",
    "            0 \\\\\n",
    "            1\n",
    "            \\end{array}\n",
    "            \\right)\n",
    "$$\n",
    "\n",
    "如何比较输出值与预期值之间的差距？ *交叉熵*。\n",
    "\n",
    "交叉熵是用来衡量两个概率分布之间的距离的函数。它是分类问题中比较常见的损失函数。其定义为\n",
    "\n",
    "$$\n",
    "H(p,q)=-\\sum_{x}p(x)log[q(x)]\n",
    "$$\n",
    "\n",
    "如何将神经网络的结果变成一个概率分布？使用softmax函数。\n",
    "\n",
    "![softchange](./softchange.png)\n",
    "\n",
    "加入神经网络的原始输出为$\\{y_1,y_2,...,y_n\\}$\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}(y)_{i}=y_{i}^{\\prime}=\\frac{e^{y i}}{\\sum_{j=1}^{n} e^{y j}}\n",
    "$$\n",
    "\n",
    "这个函数满足，概率分布的所有条件。这样就把神经网络的输出改成了一个概率分布。这样就可以计算交叉熵了。\n",
    "\n",
    "但是需要注意的一点是交叉熵并不是对称的\n",
    "\n",
    "$$\n",
    "H(p,q)\\neq H(q,p)\n",
    "$$\n",
    "\n",
    "比如我们可以这样来表述交叉熵$H(p,q)$,用q来刻画p的困哪程度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, le-10, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> [[2.5 2.5 3. ]\n",
      " [4.  4.5 4.5]]\n",
      "---> [0.        0.6931472 1.0986123]\n",
      "---> [[ 5. 12.]\n",
      " [21. 32.]]\n",
      "---> [[19. 22.]\n",
      " [43. 50.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    v = tf.constant([[1.0,2.0,3.0], [4.0,5.0,6.0]])\n",
    "    print(\"--->\",tf.clip_by_value(v, 2.5, 4.5).eval())\n",
    "    v = tf.constant([1.0, 2.0, 3.0])\n",
    "    print(\"--->\", tf.log(v).eval())\n",
    "    vl = tf.constant([[1.0, 2.0], [3.0 , 4.0]])\n",
    "    v2 = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "    print(\"--->\", (vl *v2).eval())\n",
    "    print(\"--->\", tf.matmul(vl , v2).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "\\left(\n",
    "    \\begin{array}{cc}\n",
    "    1 & 2 \\\\\n",
    "    3 & 4\n",
    "    \\end{array}\n",
    "    \\right)*\\left(\n",
    "        \\begin{array}{cc}\n",
    "        5 & 6 \\\\\n",
    "        7 & 8\n",
    "        \\end{array}\n",
    "        \\right)=\\left(\n",
    "            \\begin{array}{cc}\n",
    "            5 & 12 \\\\\n",
    "            21 & 32\n",
    "            \\end{array}\n",
    "            \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> 3.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    v = tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "    print(\"--->\", tf.reduce_mean(v).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "\\frac{1+2+3+4+5+6}{6}=3.5\n",
    "$$\n",
    "\n",
    "tensforlow 提供了连个合并softmax和交叉熵的公式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy= tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$y$ 代表神经网络的输出，而$y\\_$代表的标准答案。如果只有一个正确答案的分类问题中可以使用另外的一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy= tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 回归问题\n",
    "\n",
    "回归问题一般采用的损失函数未均方误差。\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}\\left(y, y^{\\prime}\\right)=\\frac{\\sum_{i=1}^{n}\\left(y_{i}-y_{i}^{\\prime}\\right)^{2}}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mse =  tf.reduce_mean(tf.square(y_ - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "定义神经网络的相关参数和变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "设置自定义的损失函数\n",
    "\n",
    "$$\n",
    "\\operatorname{Loss}\\left(y, y^{\\prime}\\right)=\\sum_{i=1}^{n} f\\left(y_{i}, y_{i}^{\\prime}\\right), \\quad f(x, y)=\\left\\{\\begin{array}{ll}{a(x-y)} & {x>y} \\\\ {b(y-x)} & {x \\leqslant y}\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。\n",
    "loss_less = 10\n",
    "loss_more = 1\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "vl = tf.constant([l.0, 2.0, 3.0, 4.0])\n",
    "v2 = tf.constant([4 . 0, 3 . 0, 2 . 0, 1.0])\n",
    "sess = tf.InteractiveSession()\n",
    "print(tf.greater(vl, v2) .eval()) #输出[False False True True]\n",
    "print(tf.where(tf.greater(vl, v2), vl, v2).eval()) #输出[4. 3. 3. 4.J\n",
    "sess.close ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "生成模拟数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rdm = RandomState(1)\n",
    "X = rdm.rand(128,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % 128\n",
    "        end = (i*batch_size) % 128 + batch_size\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), w1 is: \" % (i))\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "重新定义损失函数，使得预测多了的损失大，于是模型应该偏向少的方向预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y, y_)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % 128\n",
    "        end = (i*batch_size) % 128 + batch_size\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), w1 is: \" % (i))\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "不同的损失函数会对训练得到的模型产生重要影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 神经网络优化算法\n",
    "\n",
    "反向传到的推导。另立专题 FIXME\n",
    "\n",
    "## 梯度下降算法\n",
    "\n",
    "$$y=x^2$$\n",
    "\n",
    "求导数\n",
    "\n",
    "$$y^{'}=2x$$\n",
    "\n",
    "那么怎么迭代到函数最低点？\n",
    "\n",
    "1. 随即取初始值比如8，学习率0.1\n",
    "2. 迭代第一次\n",
    "\n",
    "   $$6.4=8-2\\times 8 * 0.1$$\n",
    "\n",
    "3. 迭代第二次\n",
    "\n",
    "   $$5.12=6.4-2\\times 6.4 * 0.1$$\n",
    "\n",
    "4. 迭代第第三次\n",
    "\n",
    "   $$4.096=5.12-2\\times 5.12 * 0.1$$\n",
    "\n",
    "\n",
    "是这梯度下降算法的一般更新公式\n",
    "\n",
    "$$\n",
    "\\theta_{n+1}=\\theta_{n}-\\delta \\frac{\\partial J(\\theta_n)}{\\partial \\theta_n}\n",
    "$$\n",
    "\n",
    "## 共轭梯度下降算法\n",
    "\n",
    "FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "## 问题\n",
    "\n",
    "1. 可能陷入局部最优解。\n",
    "2. 数据很大时，计算所有数据的梯度非常耗时。\n",
    "3. 如何解决（随机梯度下降）\n",
    "4. 随机梯度下降，缺点可能连局部最优都找不到。\n",
    "5. 可以把数据划分为不同的batch来训练。\n",
    "\n",
    "## 学习率\n",
    "\n",
    "1. 假设我们要最小化函数  $y=x^2$, 选择初始点   $x_0=5$\n",
    "2. 学习率为1的时候，x在5和-5之间震荡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "TRAINING_STEPS = 10\n",
    "LEARNING_RATE = 1\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        x_value = sess.run(x)\n",
    "        print \"After %s iteration(s): x%s is %f.\"% (i+1, i+1, x_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "学习率为0.001的时候，下降速度过慢，在901轮时才收敛到0.823355。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        if i % 100 == 0:\n",
    "            x_value = sess.run(x)\n",
    "            print \"After %s iteration(s): x%s is %f.\"% (i+1, i+1, x_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得不错的收敛程度\n",
    "\n",
    "$$\n",
    "decayed\\_learning\\_rate=learning\\_rate\\times decay\\_rate^{\\frac{global\\_step}{decay\\_steps}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 100\n",
    "global_step = tf.Variable(0)\n",
    "LEARNING_RATE = tf.train.exponential_decay(0.1, global_step, 1, 0.96, staircase=True)\n",
    "\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y, global_step=global_step)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        if i % 10 == 0:\n",
    "            LEARNING_RATE_value = sess.run(LEARNING_RATE)\n",
    "            x_value = sess.run(x)\n",
    "            print \"After %s iteration(s): x%s is %f, learning rate is %f.\"% (i+1, i+1, x_value, LEARNING_RATE_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "staircase 的作用\n",
    "\n",
    "![staricase](./stair.png)\n",
    "\n",
    "一般来说初始学习率、衰减系数和衰减速度都是根据经验设置的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 神经网络进一步优化\n",
    "\n",
    "## 过拟合问题\n",
    "\n",
    "![overfit](./overfit.png)\n",
    "\n",
    "如何解决过拟合问题？引入正则化。正则化的思想就是在损失函数中加入刻画模型复杂程度的指标。\n",
    "\n",
    "引入正则化只是，我们的优化不再是针对$J(\\theta)$,而是$J(\\theta)+\\lambda R(w)$,其中$R(w),刻画的是模型的复杂程度.\n",
    "\n",
    "1. L1 正则化\n",
    "\n",
    "$$\n",
    "R(w)=\\|w\\|_{1}=\\sum\\left|w_{i}\\right|\n",
    "$$\n",
    "\n",
    "2. L2 正则化\n",
    "\n",
    "$$\n",
    "R(w)=\\|w\\|_{2}^{2}=\\sum_{i}\\left|w_{i}^{2}\\right|\n",
    "$$\n",
    "\n",
    "### 两个正则化公式的特点。\n",
    "\n",
    "1. L1可以使得参数更稀疏。也就是说有很多参数可能会编程0.\n",
    "2. L2则不会。因为$0.01^2$后变得很小。对优化几乎没有贡献，参数不会那么快趋近于0.\n",
    "3. L1不可导，L2可导。\n",
    "\n",
    "### 实践中结合两种优化方式\n",
    "\n",
    "$$\n",
    "R(w)=\\sum_{i} \\alpha\\left|w_{i}\\right|+(1-\\alpha) w_{i}^{2}\n",
    "$$\n",
    "\n",
    "### 举例子\n",
    "\n",
    "1. 生成模拟数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' argument has 150 elements, which is not acceptable for use with 'x' with size 150, 'y' with size 150.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4231\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Then is 'c' acceptable as PathCollection facecolors?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4232\u001b[0;31m                 \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4233\u001b[0m                 \u001b[0mn_elem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: RGBA sequence should have length 3 or 4",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-01978048c4d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m plt.scatter(data[:,0], data[:,1], c=label,\n\u001b[0;32m---> 22\u001b[0;31m             cmap=\"RdBu\", vmin=-.2, vmax=1.2, edgecolor=\"white\")\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, data, **kwargs)\u001b[0m\n\u001b[1;32m   2860\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m         verts=verts, edgecolors=edgecolors, **({\"data\": data} if data\n\u001b[0;32m-> 2862\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2863\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   4243\u001b[0m                         \u001b[0;34m\"acceptable for use with 'x' with size {xs}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4244\u001b[0m                         \u001b[0;34m\"'y' with size {ys}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4245\u001b[0;31m                         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_elem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4246\u001b[0m                     )\n\u001b[1;32m   4247\u001b[0m                 \u001b[0;31m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'c' argument has 150 elements, which is not acceptable for use with 'x' with size 150, 'y' with size 150."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "label = []\n",
    "np.random.seed(0)\n",
    "\n",
    "# 以原点为圆心，半径为1的圆把散点划分成红蓝两部分，并加入随机噪音。\n",
    "for i in range(150):\n",
    "    x1 = np.random.uniform(-1,1)\n",
    "    x2 = np.random.uniform(0,2)\n",
    "    if x1**2 + x2**2 <= 1:\n",
    "        data.append([np.random.normal(x1, 0.1),np.random.normal(x2,0.1)])\n",
    "        label.append(0)\n",
    "    else:\n",
    "        data.append([np.random.normal(x1, 0.1), np.random.normal(x2, 0.1)])\n",
    "        label.append(1)\n",
    "data = np.hstack(data).reshape(-1,2)\n",
    "label = np.hstack(label).reshape(-1, 1)\n",
    "plt.scatter(data[:,0], data[:,1], c=label,\n",
    "            cmap=\"RdBu\", vmin=-.2, vmax=1.2, edgecolor=\"white\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "2. 定义一个获取权重，并自动加入正则项到损失的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_weight(shape, lambda1):\n",
    "    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(lambda1)(var))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "3. 定义神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 13:03:08.375648 4583294400 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "sample_size = len(data)\n",
    "\n",
    "# 每层节点的个数\n",
    "layer_dimension = [2,10,5,3,1]\n",
    "\n",
    "n_layers = len(layer_dimension)\n",
    "\n",
    "cur_layer = x\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "# 循环生成网络结构\n",
    "for i in range(1, n_layers):\n",
    "    out_dimension = layer_dimension[i]\n",
    "    weight = get_weight([in_dimension, out_dimension], 0.003)\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[out_dimension]))\n",
    "    cur_layer = tf.nn.elu(tf.matmul(cur_layer, weight) + bias)\n",
    "    in_dimension = layer_dimension[i]\n",
    "\n",
    "y= cur_layer\n",
    "\n",
    "# 损失函数的定义。\n",
    "mse_loss = tf.reduce_sum(tf.pow(y_ - y, 2)) / sample_size\n",
    "tf.add_to_collection('losses', mse_loss)  #集合的概念\n",
    "loss = tf.add_n(tf.get_collection('losses'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "4. 训练不带正则项的损失函数mse_loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 定义训练的目标函数mse_loss，训练次数及训练模型\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(mse_loss)\n",
    "TRAINING_STEPS = 40000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op, feed_dict={x: data, y_: label})\n",
    "        if i % 2000 == 0:\n",
    "            print(\"After %d steps, mse_loss: %f\" % (i,sess.run(mse_loss, feed_dict={x: data, y_: label})))\n",
    "\n",
    "    # 画出训练后的分割曲线\n",
    "    xx, yy = np.mgrid[-1.2:1.2:.01, -0.2:2.2:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    probs = sess.run(y, feed_dict={x:grid})\n",
    "    probs = probs.reshape(xx.shape)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=label,\n",
    "            cmap=\"RdBu\", vmin=-.2, vmax=1.2, edgecolor=\"white\")\n",
    "plt.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "5. 训练带正则项的损失函数loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 定义训练的目标函数loss，训练次数及训练模型\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "TRAINING_STEPS = 40000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op, feed_dict={x: data, y_: label})\n",
    "        if i % 2000 == 0:\n",
    "            print(\"After %d steps, loss: %f\" % (i, sess.run(loss, feed_dict={x: data, y_: label})))\n",
    "\n",
    "    # 画出训练后的分割曲线\n",
    "    xx, yy = np.mgrid[-1:1:.01, 0:2:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    probs = sess.run(y, feed_dict={x:grid})\n",
    "    probs = probs.reshape(xx.shape)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=label,\n",
    "            cmap=\"RdBu\", vmin=-.2, vmax=1.2, edgecolor=\"white\")\n",
    "plt.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 滑动平均模型\n",
    "\n",
    "1. 定义变量及滑动平均类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "v1 = tf.Variable(0, dtype=tf.float32)\n",
    "step = tf.Variable(0, trainable=False)\n",
    "ema = tf.train.ExponentialMovingAverage(0.99, step)\n",
    "maintain_averages_op = ema.apply([v1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "2. 查看不同迭代中变量取值的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    # 初始化\n",
    "    # 初始化之后变 v1=0,v1的滑动平均为0\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print sess.run([v1, ema.average(v1)])\n",
    "\n",
    "    # 更新变量v1的取值\n",
    "    # 更新vl的消Z)IJ平均值。衰减率为min{0.99, (1+step)/(10+step)= 0.1)=0.l,\n",
    "    # 所以 vl 的滑动平均会被更新为 0.1×0+0.9×5=4.5\n",
    "    sess.run(tf.assign(v1, 5))\n",
    "    sess.run(maintain_averages_op)\n",
    "    print sess.run([v1, ema.average(v1)])\n",
    "\n",
    "    # 更新step和v1的取值\n",
    "    # 更新 v1 的滑动平均值。衰减率为 rnin{0.99, (1+step)/(10+step)=0.999}=0 . 99 ,\n",
    "    # 所以 v1 的滑动平均会被更新为 0.99x4.5+0.01x10=4.555。\n",
    "    sess.run(tf.assign(step, 10000))\n",
    "    sess.run(tf.assign(v1, 10))\n",
    "    sess.run(maintain_averages_op)\n",
    "    print sess.run([v1, ema.average(v1)])\n",
    "\n",
    "    # 更新一次v1的滑动平均值\n",
    "    # 再次更新滑动平均值，得到的新滑动平均值为 0.99×4.555+0.01xl0=4.60945\n",
    "    sess.run(maintain_averages_op)\n",
    "    print sess.run([v1, ema.average(v1)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "ch04.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
