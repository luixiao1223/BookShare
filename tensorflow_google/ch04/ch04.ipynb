{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 深度学习与深层神经网路\n",
    "\n",
    "1. 深度学习就是深层神经网络的代名词\n",
    "2. 深度学习最重要的两个特性\n",
    "   + 多层\n",
    "   + 非线性\n",
    "\n",
    "## 线性模型的局限性\n",
    "\n",
    "最早的神经网络采用线性模型\n",
    "\n",
    "$$\n",
    "y=\\sum_{i}w_ix_i+b\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(1)}=xW^{(1)},y=a^{(1)}W^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y=(xW^{(1)})W^{(2)}\\rightarrow y=x(W^{(1)}W^{(2)})=xW^{'}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y=xW^{'}=\\left[\n",
    "      \\begin{array}{cc}\n",
    "      x_1 & x_2 \n",
    "      \\end{array}\n",
    "      \\right]\\left[\n",
    "          \\begin{array}{c}\n",
    "          W_1^{'} \\\\\n",
    "  W_2^{'} \n",
    "          \\end{array}\n",
    "          \\right]=\\left[\n",
    "              \\begin{array}{cc}\n",
    "              W_{1}^{'}x_1 & W_2^{'} x_2\n",
    "              \\end{array}\n",
    "              \\right]\n",
    "$$\n",
    "\n",
    "线性模型不能解决异或问题。\n",
    "\n",
    "![异或](./xor.jpg)\n",
    "\n",
    "## 激活函数实现去线性化\n",
    "\n",
    "如何做的？\n",
    "\n",
    "![加入非线性的激活函数图](./nonlinear.png)\n",
    "\n",
    "$$\n",
    "\\begin{aligned} A_{1} &=\\left[a_{11}, a_{12}, a_{13}\\right]=f\\left(x W^{(1)}+b\\right)=f\\left(\\left[x_{1}, x_{2}\\right]\\left[\\begin{array}{ccc}{W_{1,1}^{(1)}} & {W_{1,2}^{(1)}} & {W_{1,3}^{(1)}} \\\\ {W_{2,1}^{(1)}} & {W_{2,2}^{(1)}} & {W_{2,3}^{(1)}}\\end{array}\\right]+\\left[\\begin{array}{lll}{b_{1}} & {b_{2}} & {b_{3}}\\end{array}\\right]\\right) \\\\ &=f\\left(\\left[W_{1,1}^{(1)} x_{1}+W_{2,1}^{(1)} x_{2}+b_{1}, W_{1,2}^{(1)} x_{1}+W_{2,2}^{(1)} x_{2}+b_{2}, W_{1,3}^{(1)} x_{1}+W_{2,3}^{(1)} x_{2}+b_{3}\\right]\\right) \\\\ &=\\left[f\\left(W_{1,1}^{(1)} x_{1}+W_{2,1}^{(1)} x_{2}+b_{1}\\right), f\\left(W_{1,2}^{(1)} x_{1}+W_{2,2}^{(1)} x_{2}+b_{2}\\right), f\\left(W_{1,3}^{(1)} x_{1}+W_{2,3}^{(1)} x_{2}+b_{3}\\right)\\right] \\end{aligned}\n",
    "$$\n",
    "\n",
    "常用的激活函数\n",
    "\n",
    "![常用的激活函数](./act.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.nn.relu(tf.matmul(x,w1)+base1)\n",
    "b = tf.nn.relu(tf.matmul(a,w2)+base2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "## 多层神经网络解决异或语言\n",
    "\n",
    "感知机理论上不可以的原因？（这个可以列一个专题来讲）FIXME 参考书籍《Perceptions:An Introtudction to Computational Geometry》 MIT Press,1969\n",
    "\n",
    "![perception](./perc.png)\n",
    "\n",
    "![deep](./deep.png)\n",
    "\n",
    "可以看到通过隐藏层，我们可以抽象出更为高维的信息。这些信息就可以用来分类数据。从而得到更好的分类结果。\n",
    "\n",
    "# 损失函数\n",
    "\n",
    "## 经典损失函数\n",
    "\n",
    "\n",
    "### 分类问题\n",
    "\n",
    "神经网络如何输出多分类问题。比如3分类问题。苹果、香蕉、梨。\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{苹果}=\\left(\n",
    "    \\begin{array}{c}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0 \n",
    "    \\end{array}\n",
    "    \\right),\\mbox{香蕉}=\\left(\n",
    "        \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        1 \\\\\n",
    "        0 \n",
    "        \\end{array}\n",
    "        \\right),\\mbox{梨}=\\left(\n",
    "            \\begin{array}{c}\n",
    "            0 \\\\\n",
    "            0 \\\\\n",
    "            1 \n",
    "            \\end{array}\n",
    "            \\right)\n",
    "$$\n",
    "\n",
    "如何比较输出值与预期值之间的差距？ *交叉熵*。\n",
    "\n",
    "交叉熵是用来衡量两个概率分布之间的距离的函数。它是分类问题中比较常见的损失函数。其定义为\n",
    "\n",
    "$$\n",
    "H(p,q)=-\\sum_{x}p(x)log[q(x)]\n",
    "$$\n",
    "\n",
    "如何将神经网络的结果变成一个概率分布？使用softmax函数。\n",
    "\n",
    "![softchange](./softchange.png)\n",
    "\n",
    "加入神经网络的原始输出为$\\{y_1,y_2,...,y_n\\}$\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}(y)_{i}=y_{i}^{\\prime}=\\frac{e^{y i}}{\\sum_{j=1}^{n} e^{y j}}\n",
    "$$\n",
    "\n",
    "这个函数满足，概率分布的所有条件。这样就把神经网络的输出改成了一个概率分布。这样就可以计算交叉熵了。\n",
    "\n",
    "但是需要注意的一点是交叉熵并不是对称的\n",
    "\n",
    "$$\n",
    "H(p,q)\\neq H(q,p)\n",
    "$$\n",
    "\n",
    "比如我们可以这样来表述交叉熵$H(p,q)$,用q来刻画p的困哪程度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, le-10, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> [[2.5 2.5 3. ]\n",
      " [4.  4.5 4.5]]\n",
      "---> [0.        0.6931472 1.0986123]\n",
      "---> [[ 5. 12.]\n",
      " [21. 32.]]\n",
      "---> [[19. 22.]\n",
      " [43. 50.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    v = tf.constant([[1.0,2.0,3.0], [4.0,5.0,6.0]])\n",
    "    print(\"--->\",tf.clip_by_value(v, 2.5, 4.5).eval())\n",
    "    v = tf.constant([1.0, 2.0, 3.0])\n",
    "    print(\"--->\", tf.log(v).eval())\n",
    "    vl = tf.constant([[1.0, 2.0], [3.0 , 4.0]])\n",
    "    v2 = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "    print(\"--->\", (vl *v2).eval())\n",
    "    print(\"--->\", tf.matmul(vl , v2).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "\\left(\n",
    "    \\begin{array}{cc}\n",
    "    1 & 2 \\\\\n",
    "    3 & 4 \n",
    "    \\end{array}\n",
    "    \\right)*\\left(\n",
    "        \\begin{array}{cc}\n",
    "        5 & 6 \\\\\n",
    "        7 & 8 \n",
    "        \\end{array}\n",
    "        \\right)=\\left(\n",
    "            \\begin{array}{cc}\n",
    "            5 & 12 \\\\\n",
    "            21 & 32 \n",
    "            \\end{array}\n",
    "            \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> 3.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    v = tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "    print(\"--->\", tf.reduce_mean(v).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "\\frac{1+2+3+4+5+6}{6}=3.5\n",
    "$$\n",
    "\n",
    "tensforlow 提供了连个合并softmax和交叉熵的公式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy= tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$y$ 代表神经网络的输出，而$y\\_$代表的标准答案。如果只有一个正确答案的分类问题中可以使用另外的一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy= tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 回归问题\n",
    "\n",
    "回归问题一般采用的损失函数未均方误差。\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}\\left(y, y^{\\prime}\\right)=\\frac{\\sum_{i=1}^{n}\\left(y_{i}-y_{i}^{\\prime}\\right)^{2}}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mse =  tf.reduce_mean(tf.square(y_ - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "定义神经网络的相关参数和变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name=\"x-input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')\n",
    "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "设置自定义的损失函数\n",
    "\n",
    "$$\n",
    "\\operatorname{Loss}\\left(y, y^{\\prime}\\right)=\\sum_{i=1}^{n} f\\left(y_{i}, y_{i}^{\\prime}\\right), \\quad f(x, y)=\\left\\{\\begin{array}{ll}{a(x-y)} & {x>y} \\\\ {b(y-x)} & {x \\leqslant y}\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。\n",
    "loss_less = 10\n",
    "loss_more = 1\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "vl = tf.constant([l.0, 2.0, 3.0, 4.0])\n",
    "v2 = tf.constant([4 . 0, 3 . 0, 2 . 0, 1.0])\n",
    "sess = tf.InteractiveSession()\n",
    "print(tf.greater(vl, v2) .eval()) #输出[False False True True]\n",
    "print(tf.where(tf.greater(vl, v2), vl, v2).eval()) #输出[4. 3. 3. 4.J\n",
    "sess.close ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "生成模拟数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rdm = RandomState(1)\n",
    "X = rdm.rand(128,2)\n",
    "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % 128\n",
    "        end = (i*batch_size) % 128 + batch_size\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), w1 is: \" % (i))\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "重新定义损失函数，使得预测多了的损失大，于是模型应该偏向少的方向预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(y, y_)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % 128\n",
    "        end = (i*batch_size) % 128 + batch_size\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            print(\"After %d training step(s), w1 is: \" % (i))\n",
    "            print sess.run(w1), \"\\n\"\n",
    "    print \"Final w1 is: \\n\", sess.run(w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "不同的损失函数会对训练得到的模型产生重要影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 神经网络优化算法\n",
    "\n",
    "反向传到的推导。另立专题 FIXME\n",
    "\n",
    "## 梯度下降算法\n",
    "\n",
    "## 共轭梯度下降算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "## 问题\n",
    "\n",
    "1. 可能陷入局部最优解。\n",
    "2. 数据很大时，计算所有数据的梯度非常耗时。\n",
    "3. 如何解决（随机梯度下降）\n",
    "4. 随机梯度下降，缺点可能连局部最优都找不到。\n",
    "5. 可以把数据划分为不同的batch来训练。\n",
    "\n",
    "## 学习率\n",
    "\n",
    "1. 假设我们要最小化函数  $y=x^2$, 选择初始点   $x_0=5$\n",
    "2. 学习率为1的时候，x在5和-5之间震荡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "TRAINING_STEPS = 10\n",
    "LEARNING_RATE = 1\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        x_value = sess.run(x)\n",
    "        print \"After %s iteration(s): x%s is %f.\"% (i+1, i+1, x_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "学习率为0.001的时候，下降速度过慢，在901轮时才收敛到0.823355。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        if i % 100 == 0: \n",
    "            x_value = sess.run(x)\n",
    "            print \"After %s iteration(s): x%s is %f.\"% (i+1, i+1, x_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得不错的收敛程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 100\n",
    "global_step = tf.Variable(0)\n",
    "LEARNING_RATE = tf.train.exponential_decay(0.1, global_step, 1, 0.96, staircase=True)\n",
    "\n",
    "x = tf.Variable(tf.constant(5, dtype=tf.float32), name=\"x\")\n",
    "y = tf.square(x)\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y, global_step=global_step)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        sess.run(train_op)\n",
    "        if i % 10 == 0:\n",
    "            LEARNING_RATE_value = sess.run(LEARNING_RATE)\n",
    "            x_value = sess.run(x)\n",
    "            print \"After %s iteration(s): x%s is %f, learning rate is %f.\"% (i+1, i+1, x_value, LEARNING_RATE_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "ch04.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
