{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# MNIST数据处理\n",
    "\n",
    "1. MNIST 是NIST的子一个子集。\n",
    "2. 60000张图片作为训练集，10000张作为验证集。\n",
    "3. $28x28$图片\n",
    "\n",
    "## 看具体代码\n",
    "\n",
    "tensorflow 具有处理MNIST数据的程序包。具体运用如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# 如果对应文件夹没有数据，则会下载。\n",
    "mnist = input_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "\n",
    "# 其中的one_hot参数就是说一个label中只有一个1.\n",
    "# 比如(0,0,0,1)就是one_hot，而(0,1,1,0)就不是one_hot\n",
    "\n",
    "# 打印训练数据大小\n",
    "print(\"Training data size \", mnist.train.num_examples)\n",
    "\n",
    "# 打印验证数据大小\n",
    "print(\"Validation data size \", mnist.validation.num_examples)\n",
    "\n",
    "# 打印测试数据大小\n",
    "print(\"Testing data size\", mnist.test.num_examples)\n",
    "\n",
    "# print an example training data\n",
    "print(\"An example training data:\", mnist.train.images[0])\n",
    "\n",
    "# print example training data label\n",
    "print(\"Example training data label\", mnist.train.labels[0])\n",
    "\n",
    "#可以设置取数据的batch大小\n",
    "\n",
    "batch_size = 10\n",
    "xs, ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "print(\"X Shape\", xs.shape)\n",
    "print(\"Y Shape\", ys.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 训练代码(一个完整的示例)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# MNIST data set related constant\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# neural network parameters\n",
    "LAYER1_NODE = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "\n",
    "REGULARAZTION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# auxiliary functions\n",
    "\n",
    "\n",
    "def inference(input_tensor, avg_class, weight1, biases1,\n",
    "              weights2, biases2):\n",
    "    if avg_class is None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weight1)+biases1)\n",
    "        return tf.matmul(layer1, weights2)+biases2\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(\n",
    "            tf.matmul(input_tensor, avg_class.average(weight1)) +\n",
    "            avg_class.average(biases1))\n",
    "        return (tf.matmul(layer1, avg_class.average(weights2)) +\n",
    "                avg_class.average(biases2))\n",
    "\n",
    "# The training process\n",
    "\n",
    "\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    #tf.truncated_normal 如果取到的值超过两个标准差，则重新pick\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    # 生成网络,但是不使用滑动平均，所以传入None\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 定义存储训练轮次的变量。这个变量不需要计算滑动平均。所以设置为(trainable=False)。\n",
    "    # 训练轮次一般设置为不可训练参数\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义滑动平均类\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "    # 对训练图上可训练的参数所有全部应用滑动平均,其中tf.trainable_variables返回的是\n",
    "    # GraphKeys.TRAINABLE_VARIABLES中的元素，也就是那些没有被设置trainable=False的\n",
    "    # 参数。\n",
    "    variables_averages_op = variable_averages.apply(\n",
    "        tf.trainable_variables())\n",
    "\n",
    "    # 生成网络，这次使用滑动平均,因为上一步是应用滑动平均到变量上，本质是计算一个影子变量。\n",
    "    # 要想使用这个影子变量，必须认为应用它，也就是这里设置variable_averages的原因。\n",
    "    average_y = inference(\n",
    "        x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 计算交叉熵,tf.argmax 会给出向量中最大值的坐标位置。(2,3,4,5,1) -> 3\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=y, labels=tf.argmax(y_, 1))\n",
    "\n",
    "    # 计算一个batch中交叉熵的均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # 计算损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    regularaztion = regularizer(weights1) + regularizer(weights2)\n",
    "    loss = cross_entropy_mean + regularaztion\n",
    "\n",
    "    # 设置指数衰减的学习率。\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    # 优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 在训练神经网络模型时，每过一遍数据既需要通过反向传播来更新神经网络中的参数，\n",
    "    # 又要更新每一个参数的滑动平均值。为了一次完成多个操作， TensorFlow 提供了\n",
    "    # tf.control_dependencies和tf.group两种机制。 下面两行程序和\n",
    "    # train_op = tf.group(train_step, variables_averages_op)是等价的。\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 计算正确率\n",
    "    # 在验证数据上使用滑动平均，而不是在训练数据上使用滑动平均。\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        validate_feed = {x: mnist.validation.images,\n",
    "                         y_: mnist.validation.labels}\n",
    "\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "        # 循环的训练神经网络。\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \"\n",
    "                      % (i, validate_acc))\n",
    "\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\"\n",
    "               % (TRAINING_STEPS, test_acc)))\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"./data/\",\n",
    "                                      one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 如何验证数据\n",
    "\n",
    "1. 交叉验证的方式效果比较好。\n",
    "2. 但是面对海量数据的时候，一般采用上面的验证数据集的形式来评估。\n",
    "3. 在验证数据上使用滑动平均，而不是在训练数据上使用滑动平均。\n",
    "\n",
    "原因:对神经网络边的权重 weights 使用滑动平均，得到对应的影子变量 shadow_weights。在训练过程仍然使用原来不带滑动平均的权重 weights，不然无法得到 weights 下一步更新的值，又怎么求下一步 weights 的影子变量 shadow_weights。之后在测试过程中使用 shadow_weights 来代替 weights 作为神经网络边的权重，这样在测试数据上效果更好。因为 shadow_weights 的更新更加平滑，对于随机梯度下降而言，更平滑的更新说明不会偏离最优点很远；对于梯度下降 batch gradient decent，我感觉影子变量作用不大，因为梯度下降的方向已经是最优的了，loss 一定减小；对于 mini-batch gradient decent，可以尝试滑动平均，毕竟 mini-batch gradient decent 对参数的更新也存在抖动。\n",
    "\n",
    "**验证数据集合测试数据集要有一定的一致性。模型才能有更好表现。**\n",
    "\n",
    "## 不同模型效果比较\n",
    "\n",
    "### 回归第四章的5种优化方法\n",
    "\n",
    "1. 激活函数。\n",
    "2. 多层网络。\n",
    "3. 学习率指数衰减。\n",
    "4. 正则化的损失函数。\n",
    "5. 滑动平均。\n",
    "\n",
    "### 损失函数比单独交叉熵，更能挖掘数据的潜力。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# cross-entropy\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "    cross_entropy_mean, global_step = global_step)\n",
    "\n",
    "# cross-entropy and regularization\n",
    "\n",
    "loss = cross_entropy_mean + regularaztion\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "    cross_entropy_mean, global_step = global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![picture](regu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 变量管理\n",
    "\n",
    "## 创建变量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.get_variable\n",
    "tf.variable_scope\n",
    "\n",
    "# 等价问题\n",
    "\n",
    "v = tf.get_variable(\"v\", shape=[1], initializer=tf.constant_initializer(1.0))\n",
    "v = tf.Variable(tf.constant(1.0, shape=[1]), name=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "tensorflow的初始化函数\n",
    "\n",
    "1. tf.constant_initializer\n",
    "2. tf.random_normal_initializer\n",
    "3. tf.tnmcated_norrηal_imtializer\n",
    "4. tf.random_uniform_initializer\n",
    "5. tf.uniform_unit_sealing_initializer\n",
    "6. tf.zeros_initializer\n",
    "7. tf.ones_initializer\n",
    "\n",
    "### 通过tf.variable_scope指定上下文环境\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\", [1], initializer=tf.constant_initializer(1.0))\n",
    "\n",
    "#因为在命名v存在了，所以以下代码将会报错:\n",
    "#with tf.variable_scope(\"foo\"):\n",
    "#   v = tf.get_variable(\"v\", [1]) # Error!!!\n",
    "\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "    print(v == v1)\n",
    "\n",
    "# 命名空间var中没有创建变量v，所以以下代码将会Error\n",
    "#with tf.variable_scope(\"bar\", reuse=True):\n",
    "#   v = tf.get_variable(\"v\", [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "###  嵌套上下文管理器中的reuse参数的使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"root\"):\n",
    "    print(tf.get_variable_scope().reuse)\n",
    "    with tf.variable_scope(\"foo\", reuse=True):\n",
    "        print(tf.get_variable_scope().reuse)\n",
    "        with tf.variable_scope(\"bar\"):\n",
    "            print(tf.get_variable_scope().reuse)\n",
    "    print(tf.get_variable_scope().reuse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 通过variable_scope来管理变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "v1 = tf.get_variable(\"v\", [1])\n",
    "print(v1.name)\n",
    "\n",
    "with tf.variable_scope(\"foo\",reuse=True):\n",
    "    v2 = tf.get_variable(\"v\", [1])\n",
    "    print(v2.name)\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        v3 = tf.get_variable(\"v\", [1])\n",
    "        print(v3.name)\n",
    "\n",
    "v4 = tf.get_variable(\"v1\", [1])\n",
    "print(v4.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 我们可以通过变量的名称来获取变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"\",reuse=True):\n",
    "    v5 = tf.get_variable(\"foo/bar/v\", [1])\n",
    "    print v5 == v3\n",
    "    v6 = tf.get_variable(\"v1\", [1])\n",
    "    print v6 == v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Tensorflow 模型持久化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "v1 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))\n",
    "v2 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))\n",
    "result = v1 + v2\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    saver.save(sess, \"Saved_model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "这个目录下回出现三个文件\n",
    "\n",
    "1. model.ckpt.meta 计算图结构\n",
    "2. model.ckpt 每一个变量的值\n",
    "3. checkpoint\n",
    "\n",
    "## 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "v1 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))\n",
    "v2 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))\n",
    "result = v1 + v2\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"Saved_model/model.ckpt\")\n",
    "    print sess.run(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. 两段代码的区别，第二段代码没有初始化过程。\n",
    "2. 如果不想重复定义图上的计算。可以直接加载已经持久化固化的图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 直接加载持久化的图。因为之前没有导出v3，所以这里会报错\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"Saved_model/model.ckpt.meta\")\n",
    "v3 = tf.Variable(tf.random_normal([1], stddev=1, seed=1))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"Saved_model/model.ckpt\")\n",
    "    print sess.run(v1)\n",
    "    print sess.run(v2)\n",
    "    print sess.run(v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"path/model.ckpt.meta\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"path/mode.ckpt\")\n",
    "    print(sess.run(tf.get_default_graph().get_tensor_by_name(\"add:0\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 变量重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "v1 = tf.Variable(tf.constant(1.0, shape=[1]), name = \"other-v1\")\n",
    "v2 = tf.Variable(tf.constant(2.0, shape=[1]), name = \"other-v2\")\n",
    "saver = tf.train.Saver({\"v1\": v1, \"v2\": v2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 通过重命名读取滑动平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "v = tf.Variables(0, dtype=tf.float32, name=\"v\")\n",
    "saver=tf.train.Saver({\"v/ExponentialMovingAverage\":v})\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"path/mode.ckpt\")\n",
    "    print(sess.run(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "v = tf.Variable(0, dtype=tf.float32, name=\"v\")\n",
    "ema = tf.train.ExponentialMovingAverage(0.99)\n",
    "print ema.variables_to_restore() ## 这个函数，可以自动定义前面代码定义的变量的滑动平均重命名字典。\n",
    "\n",
    "saver = tf.train.Saver({\"v/ExponentialMovingAverage\": v})\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"Saved_model/model2.ckpt\")\n",
    "    print sess.run(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 使用Saver保存，会报错所有信息。有时候不需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensforflow.python.framework import graph_util\n",
    "\n",
    "v1 = tf.Variable(tf.constant(1.0, shape=[1]), name=\"v1\")\n",
    "v2 = tf.Variable(tf.constant(2.0, shape=[1]), name=\"v2\")\n",
    "\n",
    "result = v1 + v2\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    graph_def = tf.get_default_graph().as_graph_def()\n",
    "\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, graph_def, [\"add\"])\n",
    "    with tf.gfile.GFile(\"path/combined_mode.pb\", \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "## add:0 其中add是计算节点的名字，而0表示是它的第一个输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 完成迁移学习\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    mode_filename = \"path/combined_mode.pb\"\n",
    "    with gfile.FastGFile(model_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromSTring(f.read())\n",
    "\n",
    "        result = tf.import_graph_def(graph_def, return_elements=[\"add:0\"])\n",
    "        print(sess.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 重构代码\n",
    "\n",
    "为了让程序结构更清晰，代码更为整洁易读。把程序拆解为三个部分\n",
    "\n",
    "1. mnist_inference.py\n",
    "2. mnist_train.py\n",
    "3. mnist_eval.py\n",
    "\n",
    "## mnist_inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义神经网络结构相关的参数。\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "# 通过tf.get_variable函数来获取变量。\n",
    "def get_weight_variable(shape, regularizer):\n",
    "    weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    if regularizer != None: tf.add_to_collection('losses', regularizer(weights))\n",
    "    return weights\n",
    "\n",
    "# 定义神经网络的前向传播过程。\n",
    "\n",
    "def inference(input_tensor, regularizer):\n",
    "    with tf.variable_scope('layer1'):\n",
    "\n",
    "        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('layer2'):\n",
    "        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## mnist_train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_inference\n",
    "import os\n",
    "\n",
    "# 定义神经网络结构相关的参数\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = \"MNIST_model/\"\n",
    "MODEL_NAME = \"mnist_model\"\n",
    "\n",
    "# 定义训练过程\n",
    "\n",
    "def train(mnist):\n",
    "    # 定义输入输出placeholder。\n",
    "    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = mnist_inference.inference(x, regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "\n",
    "# 主程序入口\n",
    "\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"./data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## mnist_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_inference\n",
    "import mnist_train\n",
    "\n",
    "# 每10秒加载一次最新的模型\n",
    "EVAL_INTERVAL_SECS = 10\n",
    "\n",
    "def evaluate(mnist):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')\n",
    "        validate_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "\n",
    "        y = mnist_inference.inference(x, None)\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        while True:\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                    print(\"After %s training step(s), validation accuracy = %g\" % (global_step, accuracy_score))\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "            time.sleep(EVAL_INTERVAL_SECS)\n",
    "\n",
    "# 主程序\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"./data\", one_hot=True)\n",
    "    evaluate(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 遗留问题\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "ch05.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
