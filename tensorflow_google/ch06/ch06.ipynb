{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 图像识别问题介绍及经典数据集\n",
    "\n",
    "## CIFAR数据集\n",
    "\n",
    "1. CIFAR 数据集(CIFAR-10【60000张】, CIFAR-100)\n",
    "2. 大小$32\\times 32$\n",
    "3. 人工标注的正确率94%\n",
    "\n",
    "## ImageNet\n",
    "\n",
    "1. 1500W张图片\n",
    "2. ImageNet每年都会举办比赛\n",
    "3. [ImageNet](http://image-net.org/challenges/LSVRC/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 卷积神经网络介绍\n",
    "\n",
    "1. 全连接层网络\n",
    "\n",
    "问题：参数太多\n",
    "\n",
    "2. 卷积神经网络\n",
    "\n",
    "特点：减少参数\n",
    "\n",
    "## 结构\n",
    "\n",
    "1. 输入层\n",
    "2. 卷积层\n",
    "3. 池化层\n",
    "4. 全连接层\n",
    "5. Softmax层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 卷积神经网络常用结构\n",
    "\n",
    "## 卷积层\n",
    "\n",
    "**过滤器**\n",
    "\n",
    "过滤器有三个参数，长、宽和深度\n",
    "\n",
    "1. 长宽是过滤器在输入矩阵上的大小。\n",
    "2. 深度是输出矩阵的深度。\n",
    "\n",
    "$$\n",
    "g(i)=f(\\sum_{x=1}^{2}\\sum_{y=1}^{2}\\sum_{z=1}^{3}a_{x,y,z}\\times w_{x,y,z}^i+b^i)\n",
    "$$\n",
    "\n",
    "**如何理解这个深度。深度就是一次卷积核产生的输出个数。**\n",
    "\n",
    "Q:为了避免尺寸变化可以采用边界填0补充。\n",
    "\n",
    "Q:移动步长也可以设置\n",
    "\n",
    "**卷积层** 的参数是共享的。所以一个$2\\times 2$的卷积核只有4个参数。\n",
    "\n",
    "举个例子对于输入为$5\\times 5 \\times 3$的输入。如果输出的深度为$16$那么卷积核的参数数量为\n",
    "\n",
    "$$\n",
    "5\\times 5\\times 3\\times 16 + 16 = 1216\n",
    "$$\n",
    "\n",
    "最后加的16是偏移量。\n",
    "\n",
    "下面看看具体代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 卷积核\n",
    "filter_weight = tf.get_variable(\n",
    "    \"weights\", [5,5,3,16], #5,5是长宽，3是当前层深度。16是输出层深度\n",
    "    initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "biases = tf.get_variable(\n",
    "    \"biases\", [16], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "\n",
    "conv = tf.nn.conv2d(\n",
    "    input, filter_weight, strides=[1,1,1,1], # 步长，4个维度上的步长。但是第一个和最后一个参数必须为1，因为步长只对长宽有效。\n",
    "    padding=\"SAME\") #SAME 是添加全0，VALID是不添加\n",
    "\n",
    "bias = tf.nn.bias_add(conv, biases)\n",
    "actived_conv = tf.nn.relu(bias) # relu is a activate function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 池化层\n",
    "\n",
    "**作用：**\n",
    "\n",
    "1. 非常有效的缩小矩阵的尺寸。\n",
    "2. 加快计算。\n",
    "3. 防止过拟合。\n",
    "\n",
    "和卷积层不同的是，池化层的计算更为简单。\n",
    "\n",
    "1. 最大池化层。(取最大值)\n",
    "2. 平均池化层。(取平均值)\n",
    "\n",
    "**池化层的过滤器与卷积层的过滤器不同的地方在于。池化层的过滤器之作用于当前层，不跨层。**\n",
    "\n",
    "*池化层虽然可以减小深度，但是一般不这样做*\n",
    "\n",
    "有人认为池化层对模型影响不大,参考SpringenbergJT,DosovitskiyA,BroxT,et al. Striving for Simplic。 The All Convolutional Net [J]. Eprint Arxiv, 2014.\n",
    "\n",
    "**Dropout**\n",
    "\n",
    "随机的将节点的输出改为0，避免过拟合。一般只在全连接层使用。\n",
    "\n",
    "看看代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pool = tf.nn.max_pool(\n",
    "    actived_conv, ksize=[1,3,3,1],\n",
    "    strides=[1,2,2,1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 经典卷积网络模型\n",
    "\n",
    "Q: 神经网络的组合形式那么多。具体用什么模型呢？\n",
    "\n",
    "## LeNet-5\n",
    "\n",
    "1998年提出的模型，在MNIST上正确率为99.2%. 这个模型总共7层。\n",
    "\n",
    "### 重新写inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# 设定神经网络的参数\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 5\n",
    "\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 5\n",
    "\n",
    "FC_SIZE = 512\n",
    "\n",
    "# 定义前向传播的过程\n",
    "\n",
    "def inference(input_tensor, train, regularizer):\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable(\n",
    "            \"weight\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\"bias\", [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "    with tf.variable_scope(\"layer3-conv2\"):\n",
    "        conv2_weights = tf.get_variable(\n",
    "            \"weight\", [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [CONV2_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        pool_shape = pool2.get_shape().as_list()\n",
    "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3] # 把输出拉成一个一维向量。\n",
    "        reshaped = tf.reshape(pool2, [pool_shape[0], nodes]) # pool_shape[0] 为一个batch中数据的个数.\n",
    "\n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable(\"weight\", [nodes, FC_SIZE],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable(\"bias\", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        if train: fc1 = tf.nn.dropout(fc1, 0.5) # 随机的将节点的输出改为0，避免过拟合。一般只在全连接层使用。\n",
    "\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable(\"weight\", [FC_SIZE, NUM_LABELS],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None: tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable(\"bias\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 重写train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import LeNet5_infernece\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 定义神经网络相关的参数\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 6000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# 定义训练过程\n",
    "\n",
    "def train(mnist):\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,\n",
    "            LeNet5_infernece.IMAGE_SIZE,\n",
    "            LeNet5_infernece.IMAGE_SIZE,\n",
    "            LeNet5_infernece.NUM_CHANNELS],\n",
    "        name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, LeNet5_infernece.OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = LeNet5_infernece.inference(x,False,regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                LeNet5_infernece.IMAGE_SIZE,\n",
    "                LeNet5_infernece.IMAGE_SIZE,\n",
    "                LeNet5_infernece.NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "# 程序住入口\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"./data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 所以一般神经网络的模型满足下面的正则表达式\n",
    "\n",
    "输入层->((卷积层+)->(池化层？))+->(全连接层+)\n",
    "\n",
    "### 其他类似网络\n",
    "\n",
    "1. AlexNet\n",
    "2. ZF Net\n",
    "3. VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inception-v3模型\n",
    "\n",
    "与前面的模型不同之处是这里采用并联的方式。\n",
    "\n",
    "![inception](incep.png)\n",
    "\n",
    "1. 使用不同尺寸的过滤器。\n",
    "2. 如果采用padding为0，移动步长为1，1。那么最后不同过滤器的产出维度是一样的。这样就可以在深度上把他们拼接起来。\n",
    "\n",
    "### 使用Tensorflow-Slim工具来产生 inception卷积层(因为手动国语复杂。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.variable_scope(scope_name):\n",
    "    weights = tf.get_variable(\"weight\", ...)\n",
    "    biases = tf.get_variable(\"bias\", ...)\n",
    "    conv = tf.nn.conv2d(...)\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, biases))\n",
    "\n",
    "## 上面的代码等价于\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "net = slim.conv2d(input, 32, [3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 来看看一段slim创建一部分模块的代码\n",
    "\n",
    "完整的代码请参考\n",
    "[github(slim)](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v3.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                    stride=1,padding='VALID'):\n",
    "    net = 上一层的输出节点矩阵\n",
    "    with tf.variable_scope(\"Mized_7c\"):\n",
    "        with tf.variable_scope(\"Branch_0\"):\n",
    "            branch_0 = slim.conv2d(net, 320, [1,1], scope='Conv2d_0a_1x1')\n",
    "\n",
    "        with tf.variable_scope(\"Branch_1\"):\n",
    "            branch_1 = slim.conv2d(net, 384, [1,1], sclpe=\"Conv2d_0a_1x1\")\n",
    "            #拼接\n",
    "\n",
    "            branch_1 = tf.concat(3, [slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_0b_1x3'),\n",
    "                                     slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_0c_3x1')])\n",
    "\n",
    "        with tf.variable_scope('Branch_2'):\n",
    "            branch_2 = slim.conv2d(\n",
    "                net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            branch_2 = slim.conv2d(\n",
    "                branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "            branch_2 = tf.concat(3, [\n",
    "                slim.conv2d(branch_2, 384, [1,3], scope='Conv2d_0c_1x3'),\n",
    "                slim.conv2d(branch_2, 384, [3,1], scope='Conv2d_0d_3x1')])\n",
    "\n",
    "        with tf.variable_scope('Branch_3'):\n",
    "            branch_3 = slim.avg_pool2d(\n",
    "                net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "            branch_3 = slim.conv2d(\n",
    "                branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "\n",
    "        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 卷积神经网络迁移学习\n",
    "\n",
    "提出原因：现在机器学习的模型逐年增大。有没有办法，将一个问题上训练好的模型通过简单的调整使用于一个新的问题。\n",
    "\n",
    "1. 一般来说训练数据足够的话。迁移学习不如完全重新学习。\n",
    "2. 迁移学习需要的时间和训练样本数小于完整训练。\n",
    "\n",
    "## 下面的代码实现迁移学习\n",
    "\n",
    "首先通过\n",
    "\n",
    "wget http://download.tensorflow.org/example_images/flower_photos.tgz\n",
    "tar zxf flower_photos.tgz\n",
    "\n",
    "下载数据，然后将数据转换为需要的输入格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "# 原始输入数据的目录，这个目录下有5个子目录，每个子目录底下保存这属于该\n",
    "# 类别的所有图片。\n",
    "INPUT_DATA = '../../datasets/flower_photos'\n",
    "# 输出文件地址。我们将整理后的图片数据通过numpy的格式保存。\n",
    "OUTPUT_FILE = '../../datasets/flower_processed_data.npy'\n",
    "\n",
    "# 测试数据和验证数据比例。\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TEST_PERCENTAGE = 10\n",
    "\n",
    "定义数据处理过程\n",
    "\n",
    "# 读取数据并将数据分割成训练数据、验证数据和测试数据。\n",
    "def create_image_lists(sess, testing_percentage, validation_percentage):\n",
    "    sub_dirs = [x[0] for x in os.walk(INPUT_DATA)]\n",
    "    is_root_dir = True\n",
    "\n",
    "    # 初始化各个数据集。\n",
    "    training_images = []\n",
    "    training_labels = []\n",
    "    testing_images = []\n",
    "    testing_labels = []\n",
    "    validation_images = []\n",
    "    validation_labels = []\n",
    "    current_label = 0\n",
    "\n",
    "    # 读取所有的子目录。\n",
    "    for sub_dir in sub_dirs:\n",
    "        if is_root_dir:\n",
    "            is_root_dir = False\n",
    "            continue\n",
    "\n",
    "        # 获取一个子目录中所有的图片文件。\n",
    "        extensions = ['jpg', 'jpeg', 'JPG', 'JPEG']\n",
    "        file_list = []\n",
    "        dir_name = os.path.basename(sub_dir)\n",
    "        for extension in extensions:\n",
    "            file_glob = os.path.join(INPUT_DATA, dir_name, '*.' + extension)\n",
    "            file_list.extend(glob.glob(file_glob))\n",
    "        if not file_list: continue\n",
    "        print \"processing:\", dir_name\n",
    "\n",
    "        i = 0\n",
    "        # 处理图片数据。\n",
    "        for file_name in file_list:\n",
    "            i += 1\n",
    "            # 读取并解析图片，将图片转化为299*299以方便inception-v3模型来处理。\n",
    "            image_raw_data = gfile.FastGFile(file_name, 'rb').read()\n",
    "            image = tf.image.decode_jpeg(image_raw_data)\n",
    "            if image.dtype != tf.float32:\n",
    "                image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "            image = tf.image.resize_images(image, [299, 299])\n",
    "            image_value = sess.run(image)\n",
    "\n",
    "            # 随机划分数据聚。\n",
    "            chance = np.random.randint(100)\n",
    "            if chance < validation_percentage:\n",
    "                validation_images.append(image_value)\n",
    "                validation_labels.append(current_label)\n",
    "            elif chance < (testing_percentage + validation_percentage):\n",
    "                testing_images.append(image_value)\n",
    "                testing_labels.append(current_label)\n",
    "            else:\n",
    "                training_images.append(image_value)\n",
    "                training_labels.append(current_label)\n",
    "            if i % 200 == 0:\n",
    "                print i, \"images processed.\"\n",
    "        current_label += 1\n",
    "\n",
    "    # 将训练数据随机打乱以获得更好的训练效果。\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(training_images)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(training_labels)\n",
    "\n",
    "    return np.asarray([training_images, training_labels,\n",
    "                       validation_images, validation_labels,\n",
    "                       testing_images, testing_labels])\n",
    "\n",
    "#运行数据处理过程\n",
    "with tf.Session() as sess:\n",
    "    processed_data = create_image_lists(sess, TEST_PERCENTAGE, VALIDATION_PERCENTAGE)\n",
    "    # 通过numpy格式保存处理后的数据。\n",
    "    np.save(OUTPUT_FILE, processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "运行代码，可以把输入数据划分为训练、验证、测试三个数据集。并将图片从jpg转化为inception-v3模型需要的$299\\times 299 \\times 3$的数字矩阵。\n",
    "\n",
    "## 下载google训练好的Inception-v3模型\n",
    "\n",
    "wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\n",
    "\n",
    "tar xzf inception_v3_2016_08_28.tar.gz\n",
    "\n",
    "## 通过以下代码完成迁移学习\n",
    "\n",
    "定义训练过程中将要使用到的常量。\n",
    "因为GitHub无法保存大于100M的文件，所以在运行时需要先自行从Google下载inception_v3.ckpt文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "# 加载通过TensorFlow-Slim定义好的inception_v3模型。\n",
    "import tensorflow.contrib.slim.python.slim.nets.inception_v3 as inception_v3\n",
    "\n",
    "# 处理好之后的数据文件。\n",
    "INPUT_DATA = '../../datasets/flower_processed_data.npy'\n",
    "# 保存训练好的模型的路径。\n",
    "TRAIN_FILE = 'train_dir/model'\n",
    "# 谷歌提供的训练好的模型文件地址。因为GitHub无法保存大于100M的文件，所以\n",
    "# 在运行时需要先自行从Google下载inception_v3.ckpt文件。\n",
    "CKPT_FILE = '../../datasets/inception_v3.ckpt'\n",
    "\n",
    "# 定义训练中使用的参数。\n",
    "LEARNING_RATE = 0.0001\n",
    "STEPS = 300\n",
    "BATCH = 32\n",
    "N_CLASSES = 5\n",
    "\n",
    "# 不需要从谷歌训练好的模型中加载的参数。\n",
    "CHECKPOINT_EXCLUDE_SCOPES = 'InceptionV3/Logits,InceptionV3/AuxLogits'\n",
    "# 需要训练的网络层参数明层，在fine-tuning的过程中就是最后的全联接层。\n",
    "TRAINABLE_SCOPES='InceptionV3/Logits,InceptionV3/AuxLogit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "获取所有需要从谷歌训练好的模型中加载的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_tuned_variables():\n",
    "    exclusions = [scope.strip() for scope in CHECKPOINT_EXCLUDE_SCOPES.split(',')]\n",
    "\n",
    "    variables_to_restore = []\n",
    "    # 枚举inception-v3模型中所有的参数，然后判断是否需要从加载列表中移除。\n",
    "    for var in slim.get_model_variables():\n",
    "        excluded = False\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "    return variables_to_restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "获取所有需要训练的变量列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_trainable_variables():\n",
    "    scopes = [scope.strip() for scope in TRAINABLE_SCOPES.split(',')]\n",
    "    variables_to_train = []\n",
    "\n",
    "    # 枚举所有需要训练的参数前缀，并通过这些前缀找到所有需要训练的参数。\n",
    "    for scope in scopes:\n",
    "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "        variables_to_train.extend(variables)\n",
    "    return variables_to_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "定义训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 加载预处理好的数据。\n",
    "    processed_data = np.load(INPUT_DATA)\n",
    "    training_images = processed_data[0]\n",
    "    n_training_example = len(training_images)\n",
    "    training_labels = processed_data[1]\n",
    "\n",
    "    validation_images = processed_data[2]\n",
    "    validation_labels = processed_data[3]\n",
    "\n",
    "    testing_images = processed_data[4]\n",
    "    testing_labels = processed_data[5]\n",
    "    print(\"%d training examples, %d validation examples and %d testing examples.\" % (\n",
    "        n_training_example, len(validation_labels), len(testing_labels)))\n",
    "\n",
    "    # 定义inception-v3的输入，images为输入图片，labels为每一张图片对应的标签。\n",
    "    images = tf.placeholder(tf.float32, [None, 299, 299, 3], name='input_images')\n",
    "    labels = tf.placeholder(tf.int64, [None], name='labels')\n",
    "\n",
    "    # 定义inception-v3模型。因为谷歌给出的只有模型参数取值，所以这里\n",
    "    # 需要在这个代码中定义inception-v3的模型结构。虽然理论上需要区分训练和\n",
    "    # 测试中使用到的模型，也就是说在测试时应该使用is_training=False，但是\n",
    "    # 因为预先训练好的inception-v3模型中使用的batch normalization参数与\n",
    "    # 新的数据会有出入，所以这里直接使用同一个模型来做测试。\n",
    "    with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\n",
    "        logits, _ = inception_v3.inception_v3(\n",
    "            images, num_classes=N_CLASSES, is_training=True)\n",
    "\n",
    "    trainable_variables = get_trainable_variables()\n",
    "    # 定义损失函数和训练过程。\n",
    "    tf.losses.softmax_cross_entropy(\n",
    "        tf.one_hot(labels, N_CLASSES), logits, weights=1.0)\n",
    "    total_loss = tf.losses.get_total_loss()\n",
    "    train_step = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(total_loss)\n",
    "\n",
    "    # 计算正确率。\n",
    "    with tf.name_scope('evaluation'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), labels)\n",
    "        evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 定义加载Google训练好的Inception-v3模型的Saver。\n",
    "    load_fn = slim.assign_from_checkpoint_fn(\n",
    "      CKPT_FILE,\n",
    "      get_tuned_variables(),\n",
    "      ignore_missing_vars=True)\n",
    "\n",
    "    # 定义保存新模型的Saver。\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化没有加载进来的变量。\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # 加载谷歌已经训练好的模型。\n",
    "        print('Loading tuned variables from %s' % CKPT_FILE)\n",
    "        load_fn(sess)\n",
    "\n",
    "        start = 0\n",
    "        end = BATCH\n",
    "        for i in range(STEPS):\n",
    "            _, loss = sess.run([train_step, total_loss], feed_dict={\n",
    "                images: training_images[start:end],\n",
    "                labels: training_labels[start:end]})\n",
    "\n",
    "            if i % 30 == 0 or i + 1 == STEPS:\n",
    "                saver.save(sess, TRAIN_FILE, global_step=i)\n",
    "\n",
    "                validation_accuracy = sess.run(evaluation_step, feed_dict={\n",
    "                    images: validation_images, labels: validation_labels})\n",
    "                print('Step %d: Training loss is %.1f Validation accuracy = %.1f%%' % (\n",
    "                    i, loss, validation_accuracy * 100.0))\n",
    "\n",
    "            start = end\n",
    "            if start == n_training_example:\n",
    "                start = 0\n",
    "\n",
    "            end = start + BATCH\n",
    "            if end > n_training_example:\n",
    "                end = n_training_example\n",
    "\n",
    "        # 在最后的测试数据上测试正确率。\n",
    "        test_accuracy = sess.run(evaluation_step, feed_dict={\n",
    "            images: testing_images, labels: testing_labels})\n",
    "        print('Final test accuracy = %.1f%%' % (test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "运行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "ch06.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
